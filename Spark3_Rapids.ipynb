{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-03 03:24:43--  http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000.tgz\n",
      "Resolving rapidsai-data.s3-website.us-east-2.amazonaws.com (rapidsai-data.s3-website.us-east-2.amazonaws.com)... 52.219.108.96\n",
      "Connecting to rapidsai-data.s3-website.us-east-2.amazonaws.com (rapidsai-data.s3-website.us-east-2.amazonaws.com)|52.219.108.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 470557209 (449M) [application/x-compressed-tar]\n",
      "Saving to: ‘mortgage_2000.tgz’\n",
      "\n",
      "mortgage_2000.tgz   100%[===================>] 448.76M   988KB/s    in 7m 35s  \n",
      "\n",
      "2022-02-03 03:32:19 (1010 KB/s) - ‘mortgage_2000.tgz’ saved [470557209/470557209]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download mortgage data\n",
    "!wget http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names.csv\n",
      "acq/Acquisition_2000Q4.txt\n",
      "acq/Acquisition_2000Q3.txt\n",
      "acq/Acquisition_2000Q2.txt\n",
      "acq/Acquisition_2000Q1.txt\n",
      "perf/Performance_2000Q4.txt\n",
      "perf/Performance_2000Q3.txt\n",
      "perf/Performance_2000Q2.txt\n",
      "perf/Performance_2000Q1.txt\n"
     ]
    }
   ],
   "source": [
    "# copy data to /shared-data folder\n",
    "!mkdir -p /shared-data/datasets/mortgage\n",
    "!cp mortgage_2000.tgz /shared-data/datasets/mortgage\n",
    "!tar xzvf /shared-data/datasets/mortgage/mortgage_2000.tgz -C /shared-data/datasets/mortgage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /shared-data/datasets/taxi\n",
    "!cp datasets/yellow_tripdata_2015-01.csv /shared-data/datasets/taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU resources for the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 14 04:54:41 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                    0 |\r\n",
      "| N/A   26C    P0    24W / 250W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA System Management Interface \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cudf\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950M\t/shared-data/datasets/mortgage/perf/Performance_2000Q1.txt\r\n"
     ]
    }
   ],
   "source": [
    "# To compare pandas with Rapids\n",
    "orig_perf_path='/shared-data/datasets/mortgage/perf/Performance_2000Q1.txt'\n",
    "!du -hs '/shared-data/datasets/mortgage/perf/Performance_2000Q1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152K\t/shared-data/datasets/taxi/yellow_tripdata_2015-01.csv\r\n"
     ]
    }
   ],
   "source": [
    "yellow_tripdata_path = '/shared-data/datasets/taxi/yellow_tripdata_2015-01.csv'\n",
    "!du -hs '/shared-data/datasets/taxi/yellow_tripdata_2015-01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03082561492919922\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "length = len(pandas.read_csv(yellow_tripdata_path))\n",
    "end = time.time()\n",
    "print (end-start)\n",
    "print (length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234611988067627\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "length = len(cudf.read_csv(yellow_tripdata_path))\n",
    "end = time.time()\n",
    "print (end-start)\n",
    "print (length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark \n",
    "# Start with a simple standalone config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import broadcast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "orig_perf_path='/shared-data/datasets/mortgage/perf/*'\n",
    "orig_acq_path='/shared-data/datasets/mortgage/acq/*'\n",
    "tmp_perf_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/perf/'\n",
    "tmp_acq_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/acq/'\n",
    "output_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal standalone spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.executor.cores','1')\\\n",
    "    .config('driver-memory', '10G')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://expanai-sophianypteaching-gpu-0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd1bcccae90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8377296924591064\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "import time\n",
    "start = time.time()\n",
    "data = spark._sc.parallelize(range(1, 100000000, 6))\n",
    "df = spark.createDataFrame(data, IntegerType())\n",
    "data1 = spark._sc.parallelize(range(1, 100000000, 6))\n",
    "df2 = spark.createDataFrame(data1, IntegerType())\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.73046541213989\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "inner_join = df.join(df2, df.value == df2.value).count()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [value#0], [value#2], Inner\n",
      ":- *(2) Sort [value#0 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(value#0, 200), true, [id=#99]\n",
      ":     +- *(1) Filter isnotnull(value#0)\n",
      ":        +- *(1) Scan ExistingRDD[value#0]\n",
      "+- *(4) Sort [value#2 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(value#2, 200), true, [id=#105]\n",
      "      +- *(3) Filter isnotnull(value#2)\n",
      "         +- *(3) Scan ExistingRDD[value#2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, df.value == df2.value).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.executor.cores','1')\\\n",
    "    .config('driver-memory', '10G')\\\n",
    "    .config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\\\n",
    "    .config('spark.rapids.sql.enabled','true')\\\n",
    "    .config('spark.rapids.sql.explain', 'ALL')\\\n",
    "    .config('spark.rapids.sql.concurrentGpuTasks','1')\\\n",
    "    .config('spark.rapids.memory.pinnedPool.size', '8G')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04176831245422363\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "import time\n",
    "start = time.time()\n",
    "data = spark._sc.parallelize(range(1, 100000000, 6))\n",
    "df = spark.createDataFrame(data, IntegerType())\n",
    "data1 = spark._sc.parallelize(range(1, 100000000, 6))\n",
    "df2 = spark.createDataFrame(data1, IntegerType())\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.93909645080566\n"
     ]
    }
   ],
   "source": [
    "# no huge difference, it is because main time is spent on shuffling\n",
    "start = time.time()\n",
    "inner_join = df.join(df2, df.value == df2.value).count()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "GpuColumnarToRow false\n",
      "+- GpuShuffledHashJoin [value#23], [value#25], Inner, GpuBuildRight, false\n",
      "   :- GpuShuffleCoalesce 2147483647\n",
      "   :  +- GpuColumnarExchange gpuhashpartitioning(value#23, 200), true, [id=#354]\n",
      "   :     +- GpuCoalesceBatches TargetSize(2147483647)\n",
      "   :        +- GpuFilter gpuisnotnull(value#23)\n",
      "   :           +- GpuRowToColumnar TargetSize(2147483647)\n",
      "   :              +- *(1) Scan ExistingRDD[value#23]\n",
      "   +- GpuCoalesceBatches RequireSingleBatch\n",
      "      +- GpuShuffleCoalesce 2147483647\n",
      "         +- GpuColumnarExchange gpuhashpartitioning(value#25, 200), true, [id=#360]\n",
      "            +- GpuCoalesceBatches TargetSize(2147483647)\n",
      "               +- GpuFilter gpuisnotnull(value#25)\n",
      "                  +- GpuRowToColumnar TargetSize(2147483647)\n",
      "                     +- *(2) Scan ExistingRDD[value#25]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, df.value == df2.value).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled','false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare some functions to read Fannie Mae data\n",
    "# https://nvidia.github.io/spark-rapids/docs/examples.html \n",
    "# based on Fannie Mae’s Single-Family Loan Performance Data\n",
    "# https://github.com/NVIDIA/spark-xgboost-examples/blob/spark-3/datasets/ETL/MortgageETL.ipynb\n",
    "# https://docs.rapids.ai/datasets/mortgage-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import broadcast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def _get_quarter_from_csv_file_name():\n",
    "    return substring_index(substring_index(input_file_name(), '.', 1), '_', -1)\n",
    "\n",
    "_csv_perf_schema = StructType([\n",
    "    StructField('loan_id', LongType()),\n",
    "    StructField('monthly_reporting_period', StringType()),\n",
    "    StructField('servicer', StringType()),\n",
    "    StructField('interest_rate', DoubleType()),\n",
    "    StructField('current_actual_upb', DoubleType()),\n",
    "    StructField('loan_age', DoubleType()),\n",
    "    StructField('remaining_months_to_legal_maturity', DoubleType()),\n",
    "    StructField('adj_remaining_months_to_maturity', DoubleType()),\n",
    "    StructField('maturity_date', StringType()),\n",
    "    StructField('msa', DoubleType()),\n",
    "    StructField('current_loan_delinquency_status', IntegerType()),\n",
    "    StructField('mod_flag', StringType()),\n",
    "    StructField('zero_balance_code', StringType()),\n",
    "    StructField('zero_balance_effective_date', StringType()),\n",
    "    StructField('last_paid_installment_date', StringType()),\n",
    "    StructField('foreclosed_after', StringType()),\n",
    "    StructField('disposition_date', StringType()),\n",
    "    StructField('foreclosure_costs', DoubleType()),\n",
    "    StructField('prop_preservation_and_repair_costs', DoubleType()),\n",
    "    StructField('asset_recovery_costs', DoubleType()),\n",
    "    StructField('misc_holding_expenses', DoubleType()),\n",
    "    StructField('holding_taxes', DoubleType()),\n",
    "    StructField('net_sale_proceeds', DoubleType()),\n",
    "    StructField('credit_enhancement_proceeds', DoubleType()),\n",
    "    StructField('repurchase_make_whole_proceeds', StringType()),\n",
    "    StructField('other_foreclosure_proceeds', DoubleType()),\n",
    "    StructField('non_interest_bearing_upb', DoubleType()),\n",
    "    StructField('principal_forgiveness_upb', StringType()),\n",
    "    StructField('repurchase_make_whole_proceeds_flag', StringType()),\n",
    "    StructField('foreclosure_principal_write_off_amount', StringType()),\n",
    "    StructField('servicing_activity_indicator', StringType())])\n",
    "_csv_acq_schema = StructType([\n",
    "    StructField('loan_id', LongType()),\n",
    "    StructField('orig_channel', StringType()),\n",
    "    StructField('seller_name', StringType()),\n",
    "    StructField('orig_interest_rate', DoubleType()),\n",
    "    StructField('orig_upb', IntegerType()),\n",
    "    StructField('orig_loan_term', IntegerType()),\n",
    "    StructField('orig_date', StringType()),\n",
    "    StructField('first_pay_date', StringType()),\n",
    "    StructField('orig_ltv', DoubleType()),\n",
    "    StructField('orig_cltv', DoubleType()),\n",
    "    StructField('num_borrowers', DoubleType()),\n",
    "    StructField('dti', DoubleType()),\n",
    "    StructField('borrower_credit_score', DoubleType()),\n",
    "    StructField('first_home_buyer', StringType()),\n",
    "    StructField('loan_purpose', StringType()),\n",
    "    StructField('property_type', StringType()),\n",
    "    StructField('num_units', IntegerType()),\n",
    "    StructField('occupancy_status', StringType()),\n",
    "    StructField('property_state', StringType()),\n",
    "    StructField('zip', IntegerType()),\n",
    "    StructField('mortgage_insurance_percent', DoubleType()),\n",
    "    StructField('product_type', StringType()),\n",
    "    StructField('coborrow_credit_score', DoubleType()),\n",
    "    StructField('mortgage_insurance_type', DoubleType()),\n",
    "    StructField('relocation_mortgage_indicator', StringType())])\n",
    "\n",
    "def read_perf_csv(spark, path):\n",
    "    return spark.read.format('csv') \\\n",
    "            .option('nullValue', '') \\\n",
    "            .option('header', 'false') \\\n",
    "            .option('delimiter', '|') \\\n",
    "            .schema(_csv_perf_schema) \\\n",
    "            .load(path) \\\n",
    "            .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "\n",
    "def read_acq_csv(spark, path):\n",
    "    return spark.read.format('csv') \\\n",
    "            .option('nullValue', '') \\\n",
    "            .option('header', 'false') \\\n",
    "            .option('delimiter', '|') \\\n",
    "            .schema(_csv_acq_schema) \\\n",
    "            .load(path) \\\n",
    "            .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "\n",
    "def _parse_dates(perf):\n",
    "    return perf \\\n",
    "            .withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \\\n",
    "            .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \\\n",
    "            .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \\\n",
    "            .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \\\n",
    "            .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \\\n",
    "            .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \\\n",
    "            .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \\\n",
    "            .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \\\n",
    "            .withColumn('zero_balance_effective_date', to_date(col('zero_balance_effective_date'), 'MM/yyyy'))\n",
    "\n",
    "def _create_perf_deliquency(spark, perf):\n",
    "    aggDF = perf.select(\n",
    "            col(\"quarter\"),\n",
    "            col(\"loan_id\"),\n",
    "            col(\"current_loan_delinquency_status\"),\n",
    "            when(col(\"current_loan_delinquency_status\") >= 1, col(\"monthly_reporting_period\")).alias(\"delinquency_30\"),\n",
    "            when(col(\"current_loan_delinquency_status\") >= 3, col(\"monthly_reporting_period\")).alias(\"delinquency_90\"),\n",
    "            when(col(\"current_loan_delinquency_status\") >= 6, col(\"monthly_reporting_period\")).alias(\"delinquency_180\")) \\\n",
    "                    .groupBy(\"quarter\", \"loan_id\") \\\n",
    "                    .agg(\n",
    "                            max(\"current_loan_delinquency_status\").alias(\"delinquency_12\"),\n",
    "                            min(\"delinquency_30\").alias(\"delinquency_30\"),\n",
    "                            min(\"delinquency_90\").alias(\"delinquency_90\"),\n",
    "                            min(\"delinquency_180\").alias(\"delinquency_180\")) \\\n",
    "                                    .select(\n",
    "                                            col(\"quarter\"),\n",
    "                                            col(\"loan_id\"),\n",
    "                                            (col(\"delinquency_12\") >= 1).alias(\"ever_30\"),\n",
    "                                            (col(\"delinquency_12\") >= 3).alias(\"ever_90\"),\n",
    "                                            (col(\"delinquency_12\") >= 6).alias(\"ever_180\"),\n",
    "                                            col(\"delinquency_30\"),\n",
    "                                            col(\"delinquency_90\"),\n",
    "                                            col(\"delinquency_180\"))\n",
    "    joinedDf = perf \\\n",
    "            .withColumnRenamed(\"monthly_reporting_period\", \"timestamp\") \\\n",
    "            .withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n",
    "            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n",
    "            .withColumnRenamed(\"current_loan_delinquency_status\", \"delinquency_12\") \\\n",
    "            .withColumnRenamed(\"current_actual_upb\", \"upb_12\") \\\n",
    "            .select(\"quarter\", \"loan_id\", \"timestamp\", \"delinquency_12\", \"upb_12\", \"timestamp_month\", \"timestamp_year\") \\\n",
    "            .join(aggDF, [\"loan_id\", \"quarter\"], \"left_outer\")\n",
    "\n",
    "    # calculate the 12 month delinquency and upb values\n",
    "    months = 12\n",
    "    monthArray = [lit(x) for x in range(0, 12)]\n",
    "    # explode on a small amount of data is actually slightly more efficient than a cross join\n",
    "    testDf = joinedDf \\\n",
    "            .withColumn(\"month_y\", explode(array(monthArray))) \\\n",
    "            .select(\n",
    "                    col(\"quarter\"),\n",
    "                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000) / months).alias(\"josh_mody\"),\n",
    "                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000 - col(\"month_y\")) / months).alias(\"josh_mody_n\"),\n",
    "                    col(\"ever_30\"),\n",
    "                    col(\"ever_90\"),\n",
    "                    col(\"ever_180\"),\n",
    "                    col(\"delinquency_30\"),\n",
    "                    col(\"delinquency_90\"),\n",
    "                    col(\"delinquency_180\"),\n",
    "                    col(\"loan_id\"),\n",
    "                    col(\"month_y\"),\n",
    "                    col(\"delinquency_12\"),\n",
    "                    col(\"upb_12\")) \\\n",
    "                            .groupBy(\"quarter\", \"loan_id\", \"josh_mody_n\", \"ever_30\", \"ever_90\", \"ever_180\", \"delinquency_30\", \"delinquency_90\", \"delinquency_180\", \"month_y\") \\\n",
    "                            .agg(max(\"delinquency_12\").alias(\"delinquency_12\"), min(\"upb_12\").alias(\"upb_12\")) \\\n",
    "                            .withColumn(\"timestamp_year\", floor((lit(24000) + (col(\"josh_mody_n\") * lit(months)) + (col(\"month_y\") - 1)) / lit(12))) \\\n",
    "                            .selectExpr('*', 'pmod(24000 + (josh_mody_n * {}) + month_y, 12) as timestamp_month_tmp'.format(months)) \\\n",
    "                            .withColumn(\"timestamp_month\", when(col(\"timestamp_month_tmp\") == lit(0), lit(12)).otherwise(col(\"timestamp_month_tmp\"))) \\\n",
    "                            .withColumn(\"delinquency_12\", ((col(\"delinquency_12\") > 3).cast(\"int\") + (col(\"upb_12\") == 0).cast(\"int\")).alias(\"delinquency_12\")) \\\n",
    "                            .drop(\"timestamp_month_tmp\", \"josh_mody_n\", \"month_y\")\n",
    "\n",
    "    return perf.withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n",
    "            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n",
    "            .join(testDf, [\"quarter\", \"loan_id\", \"timestamp_year\", \"timestamp_month\"], \"left\") \\\n",
    "            .drop(\"timestamp_year\", \"timestamp_month\")\n",
    "\n",
    "_name_mapping = [\n",
    "        (\"WITMER FUNDING, LLC\", \"Witmer\"),\n",
    "        (\"WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015\", \"Wells Fargo\"),\n",
    "        (\"WELLS FARGO BANK,  NA\" , \"Wells Fargo\"),\n",
    "        (\"WELLS FARGO BANK, N.A.\" , \"Wells Fargo\"),\n",
    "        (\"WELLS FARGO BANK, NA\" , \"Wells Fargo\"),\n",
    "        (\"USAA FEDERAL SAVINGS BANK\" , \"USAA\"),\n",
    "        (\"UNITED SHORE FINANCIAL SERVICES, LLC D\\\\/B\\\\/A UNITED WHOLESALE MORTGAGE\" , \"United Seq(e\"),\n",
    "        (\"U.S. BANK N.A.\" , \"US Bank\"),\n",
    "        (\"SUNTRUST MORTGAGE INC.\" , \"Suntrust\"),\n",
    "        (\"STONEGATE MORTGAGE CORPORATION\" , \"Stonegate Mortgage\"),\n",
    "        (\"STEARNS LENDING, LLC\" , \"Stearns Lending\"),\n",
    "        (\"STEARNS LENDING, INC.\" , \"Stearns Lending\"),\n",
    "        (\"SIERRA PACIFIC MORTGAGE COMPANY, INC.\" , \"Sierra Pacific Mortgage\"),\n",
    "        (\"REGIONS BANK\" , \"Regions\"),\n",
    "        (\"RBC MORTGAGE COMPANY\" , \"RBC\"),\n",
    "        (\"QUICKEN LOANS INC.\" , \"Quicken Loans\"),\n",
    "        (\"PULTE MORTGAGE, L.L.C.\" , \"Pulte Mortgage\"),\n",
    "        (\"PROVIDENT FUNDING ASSOCIATES, L.P.\" , \"Provident Funding\"),\n",
    "        (\"PROSPECT MORTGAGE, LLC\" , \"Prospect Mortgage\"),\n",
    "        (\"PRINCIPAL RESIDENTIAL MORTGAGE CAPITAL RESOURCES, LLC\" , \"Principal Residential\"),\n",
    "        (\"PNC BANK, N.A.\" , \"PNC\"),\n",
    "        (\"PMT CREDIT RISK TRANSFER TRUST 2015-2\" , \"PennyMac\"),\n",
    "        (\"PHH MORTGAGE CORPORATION\" , \"PHH Mortgage\"),\n",
    "        (\"PENNYMAC CORP.\" , \"PennyMac\"),\n",
    "        (\"PACIFIC UNION FINANCIAL, LLC\" , \"Other\"),\n",
    "        (\"OTHER\" , \"Other\"),\n",
    "        (\"NYCB MORTGAGE COMPANY, LLC\" , \"NYCB\"),\n",
    "        (\"NEW YORK COMMUNITY BANK\" , \"NYCB\"),\n",
    "        (\"NETBANK FUNDING SERVICES\" , \"Netbank\"),\n",
    "        (\"NATIONSTAR MORTGAGE, LLC\" , \"Nationstar Mortgage\"),\n",
    "        (\"METLIFE BANK, NA\" , \"Metlife\"),\n",
    "        (\"LOANDEPOT.COM, LLC\" , \"LoanDepot.com\"),\n",
    "        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2015-1\" , \"JP Morgan Chase\"),\n",
    "        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2014-1\" , \"JP Morgan Chase\"),\n",
    "        (\"JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\" , \"JP Morgan Chase\"),\n",
    "        (\"JPMORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n",
    "        (\"JP MORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n",
    "        (\"IRWIN MORTGAGE, CORPORATION\" , \"Irwin Mortgage\"),\n",
    "        (\"IMPAC MORTGAGE CORP.\" , \"Impac Mortgage\"),\n",
    "        (\"HSBC BANK USA, NATIONAL ASSOCIATION\" , \"HSBC\"),\n",
    "        (\"HOMEWARD RESIDENTIAL, INC.\" , \"Homeward Mortgage\"),\n",
    "        (\"HOMESTREET BANK\" , \"Other\"),\n",
    "        (\"HOMEBRIDGE FINANCIAL SERVICES, INC.\" , \"HomeBridge\"),\n",
    "        (\"HARWOOD STREET FUNDING I, LLC\" , \"Harwood Mortgage\"),\n",
    "        (\"GUILD MORTGAGE COMPANY\" , \"Guild Mortgage\"),\n",
    "        (\"GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\" , \"GMAC\"),\n",
    "        (\"GMAC MORTGAGE, LLC\" , \"GMAC\"),\n",
    "        (\"GMAC (USAA)\" , \"GMAC\"),\n",
    "        (\"FREMONT BANK\" , \"Fremont Bank\"),\n",
    "        (\"FREEDOM MORTGAGE CORP.\" , \"Freedom Mortgage\"),\n",
    "        (\"FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"Franklin America\"),\n",
    "        (\"FLEET NATIONAL BANK\" , \"Fleet National\"),\n",
    "        (\"FLAGSTAR CAPITAL MARKETS CORPORATION\" , \"Flagstar Bank\"),\n",
    "        (\"FLAGSTAR BANK, FSB\" , \"Flagstar Bank\"),\n",
    "        (\"FIRST TENNESSEE BANK NATIONAL ASSOCIATION\" , \"Other\"),\n",
    "        (\"FIFTH THIRD BANK\" , \"Fifth Third Bank\"),\n",
    "        (\"FEDERAL HOME LOAN BANK OF CHICAGO\" , \"Fedral Home of Chicago\"),\n",
    "        (\"FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\" , \"FDIC\"),\n",
    "        (\"DOWNEY SAVINGS AND LOAN ASSOCIATION, F.A.\" , \"Downey Mortgage\"),\n",
    "        (\"DITECH FINANCIAL LLC\" , \"Ditech\"),\n",
    "        (\"CITIMORTGAGE, INC.\" , \"Citi\"),\n",
    "        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERFIRST MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n",
    "        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERBANK MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n",
    "        (\"CHASE HOME FINANCE, LLC\" , \"JP Morgan Chase\"),\n",
    "        (\"CHASE HOME FINANCE FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"JP Morgan Chase\"),\n",
    "        (\"CHASE HOME FINANCE (CIE 1)\" , \"JP Morgan Chase\"),\n",
    "        (\"CHASE HOME FINANCE\" , \"JP Morgan Chase\"),\n",
    "        (\"CASHCALL, INC.\" , \"CashCall\"),\n",
    "        (\"CAPITAL ONE, NATIONAL ASSOCIATION\" , \"Capital One\"),\n",
    "        (\"CALIBER HOME LOANS, INC.\" , \"Caliber Funding\"),\n",
    "        (\"BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\" , \"Bishops Gate Mortgage\"),\n",
    "        (\"BANK OF AMERICA, N.A.\" , \"Bank of America\"),\n",
    "        (\"AMTRUST BANK\" , \"AmTrust\"),\n",
    "        (\"AMERISAVE MORTGAGE CORPORATION\" , \"Amerisave\"),\n",
    "        (\"AMERIHOME MORTGAGE COMPANY, LLC\" , \"AmeriHome Mortgage\"),\n",
    "        (\"ALLY BANK\" , \"Ally Bank\"),\n",
    "        (\"ACADEMY MORTGAGE CORPORATION\" , \"Academy Mortgage\"),\n",
    "        (\"NO CASH-OUT REFINANCE\" , \"OTHER REFINANCE\"),\n",
    "        (\"REFINANCE - NOT SPECIFIED\" , \"OTHER REFINANCE\"),\n",
    "        (\"Other REFINANCE\" , \"OTHER REFINANCE\")]\n",
    "\n",
    "def _create_acquisition(spark, acq):\n",
    "    nameMapping = spark.createDataFrame(_name_mapping, [\"from_seller_name\", \"to_seller_name\"])\n",
    "    return acq.join(nameMapping, col(\"seller_name\") == col(\"from_seller_name\"), \"left\") \\\n",
    "      .drop(\"from_seller_name\") \\\n",
    "      .withColumn(\"old_name\", col(\"seller_name\")) \\\n",
    "      .withColumn(\"seller_name\", coalesce(col(\"to_seller_name\"), col(\"seller_name\"))) \\\n",
    "      .drop(\"to_seller_name\") \\\n",
    "      .withColumn(\"orig_date\", to_date(col(\"orig_date\"), \"MM/yyyy\")) \\\n",
    "      .withColumn(\"first_pay_date\", to_date(col(\"first_pay_date\"), \"MM/yyyy\")) \\\n",
    "\n",
    "def run_mortgage(spark, perf, acq):\n",
    "    parsed_perf = _parse_dates(perf)\n",
    "    perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n",
    "    cleaned_acq = _create_acquisition(spark, acq)\n",
    "    return perf_deliqency.join(cleaned_acq, [\"loan_id\", \"quarter\"], \"inner\").drop(\"quarter\")\n",
    "\n",
    "orig_perf_path='/shared-data/datasets/mortgage/perf/*'\n",
    "orig_acq_path='/shared-data/datasets/mortgage/acq/*'\n",
    "tmp_perf_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/perf/'\n",
    "tmp_acq_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/acq/'\n",
    "output_path='/shared-data/datasets/mortgage/mortgage_parquet_gpu/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100007365142|01/01/2000|NULL|8.0||0.0|360.0|359.0|01/2030|0.0|0|N|NULL||||||||||||||||NULL||NULL\r\n",
      "100007365142|01/01/2001|NULL|8.0|74319.0|12.0|348.0|347.0|01/2030|0.0|0|N|NULL||||||||||||||||NULL||NULL\r\n",
      "100007365142|01/01/2002|NULL|8.0|73635.48|24.0|336.0|335.0|01/2030|0.0|0|N|NULL||||||||||||||||NULL||NULL\r\n",
      "100007365142|01/01/2003|NULL|8.0|72795.41|36.0|324.0|322.0|01/2030|0.0|0|N|NULL||||||||||||||||NULL||NULL\r\n",
      "100007365142|02/01/2000|NULL|8.0||1.0|359.0|358.0|01/2030|0.0|0|N|NULL||||||||||||||||NULL||NULL\r\n"
     ]
    }
   ],
   "source": [
    "!head /shared-data/datasets/mortgage/perf/Performance_2000Q1.txt -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.8915445804596\n"
     ]
    }
   ],
   "source": [
    "#coalesce - - Only reduces the number of partitions, no increase, no shuffle, combining existing partitions. \n",
    "#Repartition - - Can increase or decrease. Full Shuffle, In general, you can determine the number of partitions by multiplying the number of CPUs in the cluster by 2, 3, or 4\n",
    "\n",
    "# Lets transcode the data first\n",
    "start = time.time()\n",
    "# we want a few big files instead of lots of small files\n",
    "acq = read_acq_csv(spark, orig_acq_path)\n",
    "acq.repartition(12).write.parquet(tmp_acq_path, mode='overwrite')\n",
    "perf = read_perf_csv(spark, orig_perf_path)\n",
    "perf.coalesce(96).write.parquet(tmp_perf_path, mode='overwrite')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851.3035500049591\n"
     ]
    }
   ],
   "source": [
    "# Now lets actually process the data\\n\",\n",
    "start = time.time()\n",
    "\n",
    "perf = spark.read.parquet(tmp_perf_path)\n",
    "acq = spark.read.parquet(tmp_acq_path)\n",
    "out = run_mortgage(spark, perf, acq)\n",
    "out.write.parquet(output_path, mode='overwrite')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_id: long (nullable = true)\n",
      " |-- monthly_reporting_period: string (nullable = true)\n",
      " |-- servicer: string (nullable = true)\n",
      " |-- interest_rate: double (nullable = true)\n",
      " |-- current_actual_upb: double (nullable = true)\n",
      " |-- loan_age: double (nullable = true)\n",
      " |-- remaining_months_to_legal_maturity: double (nullable = true)\n",
      " |-- adj_remaining_months_to_maturity: double (nullable = true)\n",
      " |-- maturity_date: string (nullable = true)\n",
      " |-- msa: double (nullable = true)\n",
      " |-- current_loan_delinquency_status: integer (nullable = true)\n",
      " |-- mod_flag: string (nullable = true)\n",
      " |-- zero_balance_code: string (nullable = true)\n",
      " |-- zero_balance_effective_date: string (nullable = true)\n",
      " |-- last_paid_installment_date: string (nullable = true)\n",
      " |-- foreclosed_after: string (nullable = true)\n",
      " |-- disposition_date: string (nullable = true)\n",
      " |-- foreclosure_costs: double (nullable = true)\n",
      " |-- prop_preservation_and_repair_costs: double (nullable = true)\n",
      " |-- asset_recovery_costs: double (nullable = true)\n",
      " |-- misc_holding_expenses: double (nullable = true)\n",
      " |-- holding_taxes: double (nullable = true)\n",
      " |-- net_sale_proceeds: double (nullable = true)\n",
      " |-- credit_enhancement_proceeds: double (nullable = true)\n",
      " |-- repurchase_make_whole_proceeds: string (nullable = true)\n",
      " |-- other_foreclosure_proceeds: double (nullable = true)\n",
      " |-- non_interest_bearing_upb: double (nullable = true)\n",
      " |-- principal_forgiveness_upb: string (nullable = true)\n",
      " |-- repurchase_make_whole_proceeds_flag: string (nullable = true)\n",
      " |-- foreclosure_principal_write_off_amount: string (nullable = true)\n",
      " |-- servicing_activity_indicator: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perf.printSchema()\n",
    "#perf.show(5)\n",
    "#acq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
