{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:12:09.811036Z",
     "start_time": "2020-11-09T09:12:09.799035Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# enlarge cell width\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession.builder.config(\"spark.master\", 'local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Schema\n",
    "\n",
    "Brazilian E-Commerce Public Dataset by Olist\n",
    "\n",
    "https://www.kaggle.com/olistbr/brazilian-ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T02:50:02.484884Z",
     "start_time": "2020-11-09T02:50:01.978336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.option('header', 'true').csv('/shared-data/olist_order_items_dataset.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data with a proper schema\n",
    "\n",
    "`df.printSchema()` lists out the data type of each column. Directly loading our csv file into Spark creates a dataframe where all columns are set to String types. While, this is not ideal as we prefer to have numeric types for `price` and datetime types for `date`.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- order_id: string (nullable = true)\n",
    " |-- order_item_id: string (nullable = true)\n",
    " |-- product_id: string (nullable = true)\n",
    " |-- seller_id: string (nullable = true)\n",
    " |-- shipping_limit_date: string (nullable = true)\n",
    " |-- price: string (nullable = true)\n",
    " |-- freight_value: string (nullable = true)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T03:46:54.973403Z",
     "start_time": "2020-11-09T03:46:54.968702Z"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T01:43:24.983895Z",
     "start_time": "2020-11-09T01:43:24.981412Z"
    }
   },
   "source": [
    "### Let Spark to infer the schema\n",
    "\n",
    "We can use the `inferSchema` option to ask Spark to guess the best suitable schema of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T01:42:52.226975Z",
     "start_time": "2020-11-09T01:42:49.923596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_infer = spark.read.option('header', 'true').option('inferSchema', 'true').csv('/shared-data/olist_order_items_dataset.csv')\n",
    "df_infer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a user-specified schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T01:50:59.821022Z",
     "start_time": "2020-11-09T01:50:59.744520Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, TimestampType, DoubleType\n",
    "\n",
    "my_schema = StructType() \\\n",
    "    .add(\"order_id\",StringType(),True) \\\n",
    "    .add(\"order_item_id\",StringType(),True) \\\n",
    "    .add(\"product_id\",StringType(),True) \\\n",
    "    .add(\"seller_id\",StringType(),True) \\\n",
    "    .add(\"shipping_limit_date\",TimestampType(),True) \\\n",
    "    .add(\"price\",DoubleType(),True) \\\n",
    "    .add(\"freight_value\",DoubleType(),True)\n",
    "\n",
    "df_user = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(my_schema) \\\n",
    "    .csv('/shared-data/olist_order_items_dataset.csv')\n",
    "\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data transform based on columns' schema\n",
    " \n",
    " A right schema makes it much easier to process our data as we can utilize native functions Spark provides to supported data types. Check [Spark documentation](https://spark.apache.org/docs/2.3.0/api/sql/index.html)  to learn more about available functions.\n",
    " \n",
    " For example, we can extract `year` and `month` information from the `shipping_limit_date` column and add them as expanded columns in our dataframe.\n",
    " \n",
    " In Spark, adding a new column is done by calling the `withColumn` interface of a dataframe:\n",
    " \n",
    " ```\n",
    "df_user1 = df_user.withColumn('Y', year('shipping_limit_date'))\n",
    "df_user1.show(5)\n",
    "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+\n",
    "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|   Y|\n",
    "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+\n",
    "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35|  58.9|        13.29|2017|\n",
    "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13| 239.9|        19.93|2017|\n",
    "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30| 199.0|        17.87|2018|\n",
    "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|2018|\n",
    "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51| 199.9|        18.14|2017|\n",
    "\n",
    " ```\n",
    " \n",
    " If we need to add multiple columns, we can pipe a seriese of `withColumn` calls together:\n",
    " ```\n",
    " df_user1 = df_user \\\n",
    "    .withColumn('Y', year('shipping_limit_date')) \\\n",
    "    .withColumn('M', month('shipping_limit_date'))\n",
    " ```\n",
    " \n",
    " __Note__: Spark does not allow you to modify an existing dataframe in-place. Every transform creates a new dataframe, and this is why we always assign a new dataframe to the transformed result `df_user1 = df_user.withColumn(...)`. But don't worry such a behavior might waste your ram -- Spark internally maintains linkage among transformed dataframes and minize memory usage automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T03:43:36.608858Z",
     "start_time": "2020-11-09T03:43:36.378410Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_user1 = df_user \\\n",
    "    .withColumn('Y', year('shipping_limit_date')) \\\n",
    "    .withColumn('M', month('shipping_limit_date'))\n",
    "df_user1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T03:44:56.241048Z",
     "start_time": "2020-11-09T03:44:56.238409Z"
    }
   },
   "source": [
    "# Data ETL\n",
    "\n",
    "ETL refers to 'Extract, Transform, and Load', which is almost the first data requirement in any project. Spark dataframe make ETL easy as many typical operations are already abstracted and embedded into Spark dataframe interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T03:51:39.591448Z",
     "start_time": "2020-11-09T03:51:39.588577Z"
    }
   },
   "source": [
    "## Filtering\n",
    "\n",
    "`Filtering` is the most common `Extract` operation, where we want to focus only on data that meets certain criterias. \n",
    "For example, if we want to look at orders in year 2017, we can use the following filter:\n",
    "\n",
    "```\n",
    "df_user1.filter(df_user1.Y == 2017)\n",
    "```\n",
    "\n",
    "Similarly, you can also apply conditions with `greater`, `less`, or `not equal` to filter dataframes.\n",
    "\n",
    "\n",
    "__Note__: `Y` column is of `integer` type. But if you use `df_user1.filter(df_user1.Y == \"2017\")`, you might be suprised that Spark still returns you the right result. This is becuase Spark trys to cast the column into a matching type to ensure the `comparison` is executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:03:12.343288Z",
     "start_time": "2020-11-09T04:03:11.938767Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"*** Compare with an integer\")\n",
    "df_user1.filter(df_user1.Y == 2017).show(2)\n",
    "\n",
    "print(\"*** Compare with a string\")\n",
    "df_user1.filter(df_user1.Y == \"2017\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with `string` type, sometime we might need to filter them by prefixes, postfixes or particular patterns. Spark provides a few string functions to make it simple for us to express these logics.\n",
    "\n",
    "For example, if we want to work with 'order_ids' that starts with `\"2\"`, we can fitler the dataframe using the `substring` function:\n",
    "\n",
    "```\n",
    "df_user.filter(substring(df_user.order_id, 0, 1) == '2').show()\n",
    "```\n",
    "\n",
    "`substring` helps us extract a segment of the string based on the start and end location we provides. \n",
    "\n",
    "\n",
    "__<span style=\"color:red\">Quiz:</span>__: How to filter `order_id` start with `2002`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:22:25.861369Z",
     "start_time": "2020-11-09T04:22:24.248654Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "df_user.filter(substring(<FIXME>) == '2002').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we are talking about filtering rows. How should we filter the dataframe by columns, e.g. choose a subset of columns while keep every row?\n",
    "\n",
    "Column filtering can be done by the `select` interface. If we only want to keep `order_id`, `price`, and `Y`, we can `select` in the following way:\n",
    "\n",
    "```\n",
    "df_user1.select('order_id', 'price', 'Y')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:28:28.807843Z",
     "start_time": "2020-11-09T04:28:28.649098Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user1.select('order_id', 'price', 'Y').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:16:17.215136Z",
     "start_time": "2020-11-09T04:16:17.210901Z"
    }
   },
   "source": [
    "__<span style=\"color:red\">Quiz:</span>__: Try to filler the dataframe whose `order_id` starts with `2002`, and only keep the `order_id` and `price` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:31:49.389463Z",
     "start_time": "2020-11-09T04:31:49.378451Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user.filter(<FIXME>).select(<FIXME>).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "We have shown how to add columns using the `withColumn` interface previousely. Now let's have a look other transforms.\n",
    "\n",
    "The simplest transform is mathematic operations, e.g. addition, subtraction, multiplication, division, etc. Check [Spark documentation](https://spark.apache.org/docs/2.3.0/api/sql/index.html)  to learn more about available math functions.\n",
    "For example, if we want to convert the `price` to use `thousand` dollar as its unit, we can\n",
    "\n",
    "```\n",
    "df_user.withColumn('price_in_k', df_user.price/1000).show()\n",
    "```\n",
    "\n",
    "We could also combine two columns information together. For example, we can get the `overall_price` by adding `price` and `freight_value` together.\n",
    "\n",
    "```\n",
    "df_user.withColumn('overall_price', df_user.price + df_user.freight_value).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:39:37.468172Z",
     "start_time": "2020-11-09T04:39:37.099899Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user.withColumn('price_in_k', df_user.price/1000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:39:37.569665Z",
     "start_time": "2020-11-09T04:39:37.470630Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user.withColumn('overall_price', df_user.price + df_user.freight_value).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<span style=\"color:red\">Quiz:</span>__: Find the square root of each oder's price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:44:10.187926Z",
     "start_time": "2020-11-09T04:44:10.178481Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import <FIXME>\n",
    "\n",
    "df_user.withColumn('sqrt_price', <FIXME>).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember Spark does not allow in-place transform, so each of our transform needs to be expressed by adding a new column into the dataframe. The good thing is that we can always use the `coloumn` filtering to keep only the columns we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Aggregation is a special `transform`: Instead of altering cells on a per-row basis, aggregation works with a group of rows and generate new values from each group. Therefore, aggregation is always used together with the `groupby` operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:17:31.986475Z",
     "start_time": "2020-11-09T06:17:30.581883Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "\n",
    "df_user1.groupby('Y', 'M').agg(sum('price')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new column's name is \"sum(price)\", which is not convenient to use later. We could use `alias` to give the column a better name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:17:04.300312Z",
     "start_time": "2020-11-09T06:17:02.709564Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user1.groupby('Y', 'M').agg(\n",
    "    sum('price').alias('monthly_total')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:20:30.444334Z",
     "start_time": "2020-11-09T06:20:30.436343Z"
    }
   },
   "source": [
    "It is also possible to have multiple aggregation with each group. For example, we can get monthly price and monthly count using the following call\n",
    "\n",
    "```\n",
    "df_user1.groupby('Y', 'M').agg(\n",
    "    sum('price').alias('monthly_total'),\n",
    "    count('price').alias('montyly_count'),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:20:44.601649Z",
     "start_time": "2020-11-09T06:20:43.068515Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user1.groupby('Y', 'M').agg(\n",
    "    sum('price').alias('monthly_total'),\n",
    "    count('price').alias('montyly_count'),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice we use the `show` interface to have a quick look at the dataframe content. However,`show` does not do full operations on the dataframe. If we want to apply the defined operations fully to the dataframe, we need to use `triggers`. The most frequently used trigger is `count`, which tell us how many records we have in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T07:36:35.136273Z",
     "start_time": "2020-11-09T07:36:32.978154Z"
    }
   },
   "outputs": [],
   "source": [
    "df_agg = df_user1.groupby('Y', 'M').agg(\n",
    "    sum('price').alias('monthly_total'),\n",
    "    count('price').alias('monthly_count'),\n",
    ")\n",
    "\n",
    "print(df_agg.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful trigger is `collect`, which pulls data out of executors and save to the driver. For examples, we may want to have the full content of `df_agg` to see the monthly trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T07:36:38.734010Z",
     "start_time": "2020-11-09T07:36:36.820414Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_result = df_agg.collect()\n",
    "print(len(agg_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we collect back are wrapped in the `Row` class. The `Row` class makes it easier for us to address individual fields in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T07:36:41.435386Z",
     "start_time": "2020-11-09T07:36:41.430481Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in agg_result:\n",
    "    print(\"Type of the object: {}\".format(type(i)))\n",
    "    print(i)\n",
    "    print(i.Y, i.M, i.monthly_total, i.monthly_count)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an convinient function to convert a Spark dataframe into a Pandas dataframe, `to_pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T07:51:13.992644Z",
     "start_time": "2020-11-09T07:51:10.790520Z"
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = df_agg.orderBy('Y', 'M').toPandas()\n",
    "pandas_df['ym'] = pandas_df['Y'].apply(lambda x: '{:04d}'.format(x)) + '-' + pandas_df['M'].apply(lambda x: '{:02d}'.format(x))\n",
    "pandas_df.plot(x='ym', y='monthly_total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "The `Load` operation refers to save the processed data in some persistent storage engines, e.g. relational databases, key-value stores, object stores, and file systems.\n",
    "\n",
    "Spark supports write data to most of these popular engines:\n",
    "* MySQL\n",
    "* Postgresql\n",
    "* Elasticserch\n",
    "* Cassandra\n",
    "* Kafka\n",
    "* AWS S3\n",
    "* Azure Blob\n",
    "* HDFS\n",
    "* Alluxio\n",
    "\n",
    "Let's take writing to csv files as an example. \n",
    "```\n",
    "df_agg.repartition(1).write.mode('overwrite').option('header', 'true').csv('/home/jovyan/output')\n",
    "```\n",
    "where\n",
    "* `repartition(1)` is used to reduce number of output files to 1\n",
    "* `mode('overwrite')` tells Spark to delete any exisiting data in the output directory \n",
    "* `option('header', 'true')` controls if a header line should be added to the csv files\n",
    "\n",
    "You can try to modify the arguments to observe Spark's writting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:03:18.901660Z",
     "start_time": "2020-11-09T08:03:16.022199Z"
    }
   },
   "outputs": [],
   "source": [
    "df_agg.repartition(1).write.mode('overwrite').option('header', 'true').csv('/root/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:07:03.236693Z",
     "start_time": "2020-11-09T08:07:02.411310Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat output/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:07:31.533550Z",
     "start_time": "2020-11-09T08:07:31.530820Z"
    }
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "Spark is very powerful and user friendly. However, as you might already noticed, Spark dataframe API still needs us to write `code`, which can be a blocker for people who are not familiar with programming. \n",
    "\n",
    "To make Spark more accessible, Spark provides a SQL interface where most common data processing can be expressed by SQL statements. So if you have worked with any relational database before, there is almost nothing stops you to use Spark to process your data.\n",
    "\n",
    "In order to use SparkSQL, we need to register our dataframe as `TempView` first.\n",
    "\n",
    "Let's register our `df_user` dataframe as a TempView named `order_items`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:39:00.249326Z",
     "start_time": "2020-11-09T08:39:00.238267Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = df_user.createOrReplaceTempView('order_items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use standard SQL to execute ETL jobs. \n",
    "\n",
    "For example, extracting `year`, `month`, `day`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:39:04.769932Z",
     "start_time": "2020-11-09T08:39:03.667824Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select\n",
    "    shipping_limit_date\n",
    "    ,year(shipping_limit_date) as Y\n",
    "    ,month(shipping_limit_date) as M\n",
    "    ,day(shipping_limit_date) as D \n",
    "from order_items \n",
    "limit 5\n",
    "'''\n",
    "\n",
    "spark.sql(statement).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:30:29.837420Z",
     "start_time": "2020-11-09T08:30:29.827616Z"
    }
   },
   "source": [
    "Get top 5 sellers who has the largest  number of orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:58:20.070196Z",
     "start_time": "2020-11-09T08:58:18.758204Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select\n",
    "    seller_id\n",
    "    ,count(*) as total_order \n",
    "from order_items\n",
    "group by seller_id\n",
    "order by total_order desc\n",
    "'''\n",
    "\n",
    "spark.sql(statement).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get average price per order for each seller. __Note__: We use nested SQL to get the result. Spark also support CTE (Common Table Expression) which makes the statement easier to reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:19:32.498437Z",
     "start_time": "2020-11-09T09:19:31.937156Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select\n",
    "    seller_id\n",
    "    ,total_price/total_order as avg_price_per_order\n",
    "from (\n",
    "    select\n",
    "        seller_id\n",
    "        ,sum(price) as total_price\n",
    "        ,count(*) as total_order \n",
    "    from order_items\n",
    "    group by seller_id\n",
    ")\n",
    "'''\n",
    "\n",
    "spark.sql(statement).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, Spark SQL also make it possible to run complicated logics that are difficult to programm using dataframe interfaces.\n",
    "\n",
    "For example, we can use windows query to find top sellers in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:48:17.839161Z",
     "start_time": "2020-11-09T08:48:13.028431Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select\n",
    "    seller_id\n",
    "    ,Y\n",
    "    ,M\n",
    "    ,total_order\n",
    "    ,rank() over (\n",
    "        partition by seller_id order by total_order desc\n",
    "    ) as rank_total_order\n",
    "from (\n",
    "    select\n",
    "        seller_id\n",
    "        ,year(shipping_limit_date) as Y\n",
    "        ,month(shipping_limit_date) as M\n",
    "        ,count(*) as total_order\n",
    "    from order_items\n",
    "    group by Y, M, seller_id\n",
    ")\n",
    "'''\n",
    "\n",
    "spark.sql(statement).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:52:33.444776Z",
     "start_time": "2020-11-09T08:52:33.438040Z"
    }
   },
   "source": [
    "__<span style=\"color:red\">Quiz:</span>__: Replace `rank` with `dense_rank` to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:09:00.492377Z",
     "start_time": "2020-11-09T09:08:55.798764Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select\n",
    "    seller_id\n",
    "    ,Y\n",
    "    ,M\n",
    "    ,total_order\n",
    "    ,<FIXME> over (\n",
    "        partition by seller_id order by total_order desc\n",
    "    ) as rank_total_order\n",
    "from (\n",
    "    select\n",
    "        seller_id\n",
    "        ,year(shipping_limit_date) as Y\n",
    "        ,month(shipping_limit_date) as M\n",
    "        ,count(*) as total_order\n",
    "    from order_items\n",
    "    group by Y, M, seller_id\n",
    ")\n",
    "'''\n",
    "\n",
    "spark.sql(statement).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<span style=\"color:red\">Quiz:</span>__: Find the months with 2nd highest number of orders for each seller, whose total number of orders in that month is greater than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:16:49.864447Z",
     "start_time": "2020-11-09T09:16:44.677807Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "select *\n",
    "from (\n",
    "    select\n",
    "        seller_id\n",
    "        ,Y\n",
    "        ,M\n",
    "        ,total_order\n",
    "        ,rank() over (\n",
    "            partition by seller_id order by total_order desc\n",
    "        ) as rank_total_order\n",
    "    from (\n",
    "        select\n",
    "            seller_id\n",
    "            ,year(shipping_limit_date) as Y\n",
    "            ,month(shipping_limit_date) as M\n",
    "            ,count(*) as total_order\n",
    "        from order_items\n",
    "        group by Y, M, seller_id\n",
    "    )\n",
    ")\n",
    "where \n",
    "    <FIXME>\n",
    "'''\n",
    "spark.sql(statement).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:25:12.211443Z",
     "start_time": "2020-11-09T09:25:12.205130Z"
    }
   },
   "source": [
    "__<span style=\"color:red\">Quiz:</span>__: Find seller who had more than 100 orders in two consecutive months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:46:40.047539Z",
     "start_time": "2020-11-09T09:46:36.287458Z"
    }
   },
   "outputs": [],
   "source": [
    "statement= '''\n",
    "with t1 as (\n",
    "    select\n",
    "        seller_id\n",
    "        ,year(shipping_limit_date) as Y\n",
    "        ,month(shipping_limit_date) as M\n",
    "        ,count(*) as total_order\n",
    "    from order_items\n",
    "    group by Y, M, seller_id\n",
    ")\n",
    "select\n",
    "    a.seller_id\n",
    "    ,a.Y\n",
    "    ,a.M\n",
    "    ,a.total_order\n",
    "    ,b.M as prev_M\n",
    "    ,b.total_order as prev_total_order\n",
    "from \n",
    "    t1 as a\n",
    "join\n",
    "    t1 as b\n",
    "on\n",
    "    a.seller_id = b.seller_id\n",
    "\n",
    "where\n",
    "   <FIXME>\n",
    "    \n",
    "'''\n",
    "spark.sql(statement).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
